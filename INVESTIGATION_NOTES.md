# Sync Segment Ordering Investigation

## Background

We're investigating bugs described in `/home/bzimmerman/git/aranya-docs/docs/sync-segment-ordering.md`, specifically **Bug 1: Segment Index Ordering Doesn't Preserve Causality**.

## Current State

- **Git commit**: `29ba5c02` (add CappedSegmentSet for visited segment tracking)
- **Branch**: `fix-nosuchparent`
- The fix commit is `da05f7ee` but we're testing on the pre-fix code to understand the bug

## Key Files

- **Responder code**: `crates/aranya-runtime/src/sync/responder.rs`
  - `find_needed_segments()` function (lines ~298-443) is the main area of interest
  - This function traverses the graph to determine which segments to send during sync

- **Slow test case**: `crates/aranya-runtime/src/testing/testdata/slow_test_4.test`
  - 16,186 lines, generated by `generate_graph` test
  - Takes >25 seconds to run (vs typical 3-4 seconds)

## Problem

The `generate_graph` test sometimes runs quickly (~3-4 seconds) and sometimes takes a very long time (>60 seconds or hangs). This variability suggests there's a performance bug triggered by certain graph structures.

Previous investigation has pointed to **graph search** as the likely culprit. The `find_needed_segments()` function does graph traversal and may have exponential behavior in certain cases.

## Next Steps

1. **Instrument the code** to identify which part is slow:
   - Add timing around `find_needed_segments()`
   - Add timing around the `is_ancestor()` calls
   - Add timing around the traversal loop
   - Count iterations/operations to identify exponential growth

2. **Run the slow test** with instrumentation:
   ```bash
   TMPDIR=/dev/shm cargo test -p aranya-runtime test_scenario_file -- slow_test_4 --nocapture
   ```
   (Note: First register `slow_test_4` in `dsl.rs` test_vectors!)

3. **Compare with fast test** to understand what graph structure triggers the slowdown

## Relevant Code Section

In `responder.rs`, the `find_needed_segments()` function:
- Traverses from the head of the graph backwards
- Uses `is_ancestor()` checks which may be expensive
- Has a `visited` set (CappedSegmentSet) to prevent revisiting segments
- The fix in `da05f7ee` added:
  - Sorting by `max_cut` instead of segment index
  - Continued traversal to prior segments even with partial coverage
  - Implied ancestry handling

## Commands to Continue

```bash
# Go to the right commit
git checkout 29ba5c02

# Register the slow test (add to test_vectors! in dsl.rs):
# slow_test_4,

# Run the slow test
TMPDIR=/dev/shm cargo test -p aranya-runtime test_scenario_file -- slow_test_4 --nocapture --test-threads=1
```

## Root Cause Analysis (2026-01-27)

### Confirmed Root Cause

The slowdown is caused by **O(traversal_segments × have_locations) calls to `is_ancestor()`** in `find_needed_segments()` (lines 356-361).

### Evidence from Instrumentation

Ran test for 20 seconds with tracing instrumentation:

1. **~62,000+ `is_ancestor` calls** executed before timeout
2. **~549 calls** to `find_needed_segments()`
3. Most expensive traversals: **1000-1300 segments** visited
4. Each `is_ancestor()` call averages **3.2 BFS iterations** (due to effective max_cut pruning)
5. Each `is_ancestor()` call does file I/O via `get_segment()` → `pread()` syscalls

### Code Path Analysis

In `find_needed_segments()` at `responder.rs:356-361`:
```rust
for &have_location in &have_locations {
    let have_segment = storage.get_segment(have_location)?;
    if storage.is_ancestor(head, &have_segment)? {
        continue 'heads;
    }
}
```

For each segment `head` visited during backward traversal:
- Calls `is_ancestor(head, have_segment)` for **every** `have_location`
- Even with 1 have_location, that's 1000+ `is_ancestor` calls per `find_needed_segments`
- Each `is_ancestor` does BFS traversal + file I/O

### Complexity

```
Total work = calls_to_find_needed_segments × segments_per_traversal × have_locations × bfs_iterations
           = 549 × 1309 × 1+ × 3.2
           ≈ 2.3 million operations minimum
```

### Why CappedSegmentSet Doesn't Help

The `visited` set in `find_needed_segments()`:
- Only prevents revisiting segments **within a single call**
- Does NOT cache `is_ancestor` results
- Does NOT help across multiple sync operations

### Potential Solutions

1. **Cache `is_ancestor` results** - Use a bounded LRU cache for (location, segment) → bool
2. **Use max_cut for early termination** - If head.max_cut < min(have_locations.max_cut), skip is_ancestor calls
3. **Batch is_ancestor queries** - Instead of checking each have_location individually, compute reachability once
4. **Incremental traversal** - Track "known ancestor" frontier instead of re-checking from scratch

## Original Hypothesis

The slowdown is likely caused by one of:
1. ~~Exponential graph traversal without proper visited tracking~~ (Ruled out - visited set works)
2. **Expensive `is_ancestor()` calls being made repeatedly** ✓ CONFIRMED
3. ~~The traversal not terminating early when it should~~ (Partially - could prune more with max_cut)

The `CappedSegmentSet` was added to help with visited tracking, but there may still be issues.
